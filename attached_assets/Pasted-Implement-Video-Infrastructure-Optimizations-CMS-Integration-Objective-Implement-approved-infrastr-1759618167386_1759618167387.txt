Implement Video Infrastructure Optimizations & CMS Integration
Objective
Implement approved infrastructure optimizations for the Ode Islands video pipeline, ensure Cloud Run scaling is properly configured, and connect the encoding pipeline to the CMS media upload function.
Task List
1. Cloud Run Configuration (4 vCPU / 8-16GiB)
Update Cloud Run deployment configuration:
yaml# cloudrun.yaml or deployment config
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: video-transcoder
spec:
  template:
    metadata:
      annotations:
        run.googleapis.com/execution-environment: gen2
        run.googleapis.com/cpu-throttling: "false"
    spec:
      containerConcurrency: 10
      resources:
        limits:
          cpu: "4"
          memory: "16Gi"
        requests:
          cpu: "4"
          memory: "8Gi"
      timeoutSeconds: 3600  # 1 hour for long videos
Update the Cloud Run service with proper resource allocation:
bash# Deploy script or terraform
gcloud run deploy video-transcoder \
  --memory=16Gi \
  --cpu=4 \
  --timeout=3600 \
  --max-instances=10 \
  --min-instances=0 \
  --concurrency=10 \
  --cpu-boost \
  --execution-environment=gen2
2. Parallel Transcoding Implementation
Update resources/cloud-transcoder/transcode.py:
pythonimport concurrent.futures
import os
from threading import Semaphore

# Resource management to prevent OOM
MAX_PARALLEL_JOBS = 4  # With 16GiB RAM, safe for 4 parallel HD transcodes
memory_semaphore = Semaphore(MAX_PARALLEL_JOBS)

def generate_variant_with_resource_management(input_file, output_dir, profile):
    """Wrapper to manage resources during parallel transcoding"""
    with memory_semaphore:  # Acquire semaphore before processing
        return generate_variant(input_file, output_dir, profile)

def process_video(input_uri, video_id):
    """Process video with parallel transcoding"""
    
    # ... existing setup code ...
    
    # Group profiles by priority for smarter processing
    critical_profiles = [p for p in applicable_profiles if p['height'] <= 480]  # 144p-480p
    standard_profiles = [p for p in applicable_profiles if 480 < p['height'] <= 1080]  # 540p-1080p
    premium_profiles = [p for p in applicable_profiles if p['height'] > 1080]  # 1440p-4K
    
    print(f"Generating {len(applicable_profiles)} quality variants in parallel...")
    
    # Process in priority order with controlled parallelism
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_PARALLEL_JOBS) as executor:
        all_futures = []
        
        # Start critical profiles first (for quick playback availability)
        critical_futures = [
            executor.submit(generate_variant_with_resource_management, input_file, output_dir, profile)
            for profile in critical_profiles
        ]
        all_futures.extend(critical_futures)
        
        # Wait for at least one critical profile to complete before starting others
        concurrent.futures.wait(critical_futures, return_when=concurrent.futures.FIRST_COMPLETED)
        
        # Now process standard profiles
        standard_futures = [
            executor.submit(generate_variant_with_resource_management, input_file, output_dir, profile)
            for profile in standard_profiles
        ]
        all_futures.extend(standard_futures)
        
        # Finally, premium profiles
        premium_futures = [
            executor.submit(generate_variant_with_resource_management, input_file, output_dir, profile)
            for profile in premium_profiles
        ]
        all_futures.extend(premium_futures)
        
        # Collect all results
        results = []
        for future in concurrent.futures.as_completed(all_futures):
            try:
                result = future.result(timeout=600)  # 10 min timeout per variant
                results.append(result)
                
                # Update status in GCS metadata for real-time progress
                update_processing_status(video_id, len(results), len(applicable_profiles))
                
            except Exception as e:
                print(f"Failed to generate variant: {e}")
                results.append(False)
    
    successful = sum(1 for r in results if r)
    print(f"✅ Generated {successful}/{len(applicable_profiles)} variants")
    
    if successful == 0:
        raise Exception("All transcoding variants failed")
    
    # Continue with at least some successful variants
    return True

def update_processing_status(video_id, completed, total):
    """Update real-time processing status in GCS metadata"""
    metadata = {
        "status": "processing",
        "progress": f"{completed}/{total}",
        "percentage": (completed / total) * 100
    }
    
    # Write to GCS metadata file
    write_gcs_metadata(f"videos/{video_id}/status.json", metadata)
3. Smart Buffer Cleanup Implementation
Update src/components/Player.tsx:
typescript// Smart buffer cleanup based on actual buffer size
const BUFFER_CLEANUP_INTERVAL = isMobile ? 2 * 60 * 1000 : 3 * 60 * 1000;
const MAX_BUFFER_SIZE_SECONDS = isMobile ? 60 : 120;  // Max buffer before forced cleanup
const MIN_BUFFER_SIZE_SECONDS = 30;  // Keep at least this much buffer

const cleanupBuffer = useCallback(() => {
  if (!hlsRef.current) return;
  
  const video = videoRef.current;
  if (!video) return;
  
  try {
    const currentTime = video.currentTime;
    const buffered = video.buffered;
    
    if (buffered.length === 0) return;
    
    // Calculate total buffer size
    let totalBufferSize = 0;
    let bufferBehind = 0;
    
    for (let i = 0; i < buffered.length; i++) {
      const start = buffered.start(i);
      const end = buffered.end(i);
      totalBufferSize += (end - start);
      
      // Calculate buffer behind current position
      if (end < currentTime) {
        bufferBehind += (end - start);
      } else if (start < currentTime && end >= currentTime) {
        bufferBehind += (currentTime - start);
      }
    }
    
    console.log(`Buffer stats - Total: ${totalBufferSize.toFixed(1)}s, Behind: ${bufferBehind.toFixed(1)}s`);
    
    // Smart cleanup: Only clean if buffer exceeds threshold
    if (totalBufferSize > MAX_BUFFER_SIZE_SECONDS || bufferBehind > MIN_BUFFER_SIZE_SECONDS) {
      // Calculate safe cleanup point
      const cleanupPoint = Math.max(0, currentTime - MIN_BUFFER_SIZE_SECONDS);
      
      // Use HLS.js API to trigger cleanup
      if (hlsRef.current.trigger) {
        hlsRef.current.trigger('hlsBufferFlushing', {
          startOffset: 0,
          endOffset: cleanupPoint,
          type: 'video'
        });
        console.log(`Cleaned buffer up to ${cleanupPoint.toFixed(1)}s`);
      }
      
      // Track cleanup metrics
      trackMetric('buffer_cleanup', {
        total_buffer: totalBufferSize,
        buffer_behind: bufferBehind,
        cleanup_point: cleanupPoint,
        device_type: isMobile ? 'mobile' : 'desktop'
      });
    }
  } catch (err) {
    console.error('Buffer cleanup error:', err);
  }
}, [isMobile, trackMetric]);

// Set up cleanup interval with smart gating
useEffect(() => {
  if (!hlsRef.current) return;
  
  const interval = setInterval(() => {
    cleanupBuffer();
  }, BUFFER_CLEANUP_INTERVAL);
  
  // Also cleanup on quality switches (high memory events)
  hlsRef.current.on(Hls.Events.LEVEL_SWITCHED, () => {
    setTimeout(cleanupBuffer, 5000);  // Cleanup 5s after quality switch
  });
  
  return () => clearInterval(interval);
}, [cleanupBuffer]);
4. CMS Media Upload Integration
Connect the encoding pipeline to CMS media upload:
typescript// src/app/api/cms/media/upload/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { Storage } from '@google-cloud/storage';
import { v4 as uuidv4 } from 'uuid';

const storage = new Storage();
const INPUT_BUCKET = 'ode-islands-video-input';
const CDN_BUCKET = 'ode-islands-video-cdn';

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData();
    const file = formData.get('file') as File;
    const metadata = JSON.parse(formData.get('metadata') as string || '{}');
    
    if (!file) {
      return NextResponse.json({ error: 'No file provided' }, { status: 400 });
    }
    
    // Generate unique video ID
    const videoId = uuidv4();
    const fileName = `${videoId}_${file.name}`;
    
    // Upload to input bucket for processing
    const bucket = storage.bucket(INPUT_BUCKET);
    const blob = bucket.file(`pending/${fileName}`);
    
    // Create write stream with metadata
    const stream = blob.createWriteStream({
      metadata: {
        contentType: file.type,
        metadata: {
          originalName: file.name,
          videoId: videoId,
          uploadedAt: new Date().toISOString(),
          cms: 'true',
          ...metadata
        }
      }
    });
    
    // Convert file to buffer and upload
    const buffer = Buffer.from(await file.arrayBuffer());
    
    await new Promise((resolve, reject) => {
      stream.on('error', reject);
      stream.on('finish', resolve);
      stream.end(buffer);
    });
    
    // Trigger Cloud Run transcoding (via Pub/Sub or direct invocation)
    await triggerTranscoding(videoId, `pending/${fileName}`);
    
    // Return immediately with processing status URL
    return NextResponse.json({
      success: true,
      videoId: videoId,
      status: 'processing',
      statusUrl: `/api/video-status/${videoId}`,
      estimatedTime: estimateProcessingTime(file.size),
      playbackUrl: `${process.env.NEXT_PUBLIC_CDN_URL}/videos/${videoId}/manifest/master.m3u8`
    });
    
  } catch (error) {
    console.error('Media upload error:', error);
    return NextResponse.json(
      { error: 'Failed to upload media' },
      { status: 500 }
    );
  }
}

async function triggerTranscoding(videoId: string, inputPath: string) {
  // Option 1: Direct Cloud Run invocation
  const response = await fetch(process.env.CLOUD_RUN_TRANSCODER_URL!, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${await getAuthToken()}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      videoId,
      inputBucket: INPUT_BUCKET,
      inputPath,
      outputBucket: CDN_BUCKET
    })
  });
  
  if (!response.ok) {
    throw new Error(`Transcoding trigger failed: ${response.statusText}`);
  }
  
  // Option 2: Pub/Sub trigger (alternative)
  // await publishTranscodingJob({ videoId, inputPath });
}

function estimateProcessingTime(fileSize: number): number {
  // Rough estimate: 1MB = 1 second of processing with parallel transcoding
  const baseTime = fileSize / (1024 * 1024);  // seconds
  return Math.min(Math.max(baseTime, 30), 3600);  // Min 30s, max 1 hour
}
5. Load Testing & Monitoring
Create load testing script:
python# load_test/test_transcoding_load.py
import asyncio
import aiohttp
import time
import psutil
import statistics

async def upload_test_video(session, file_path, endpoint):
    """Upload a test video and measure response"""
    start_time = time.time()
    
    with open(file_path, 'rb') as f:
        data = aiohttp.FormData()
        data.add_field('file', f, filename='test.mp4')
        data.add_field('metadata', '{"test": true}')
        
        async with session.post(endpoint, data=data) as response:
            result = await response.json()
            duration = time.time() - start_time
            
            return {
                'status': response.status,
                'duration': duration,
                'video_id': result.get('videoId')
            }

async def monitor_system_resources():
    """Monitor system resources during load test"""
    metrics = []
    
    for _ in range(60):  # Monitor for 60 seconds
        metrics.append({
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'memory_mb': psutil.virtual_memory().used / (1024 * 1024)
        })
        await asyncio.sleep(1)
    
    return metrics

async def run_load_test(concurrent_uploads=10, total_uploads=50):
    """Run load test with specified concurrency"""
    
    endpoint = 'http://localhost:3000/api/cms/media/upload'
    test_file = 'test_videos/sample_1min.mp4'
    
    async with aiohttp.ClientSession() as session:
        # Start resource monitoring
        monitor_task = asyncio.create_task(monitor_system_resources())
        
        # Run concurrent uploads
        tasks = []
        for i in range(0, total_uploads, concurrent_uploads):
            batch = [
                upload_test_video(session, test_file, endpoint)
                for _ in range(min(concurrent_uploads, total_uploads - i))
            ]
            
            batch_results = await asyncio.gather(*batch)
            tasks.extend(batch_results)
            
            # Add delay between batches to simulate real usage
            await asyncio.sleep(2)
        
        # Get monitoring results
        resources = await monitor_task
        
        # Analyze results
        successful_uploads = [t for t in tasks if t['status'] == 200]
        failed_uploads = [t for t in tasks if t['status'] != 200]
        response_times = [t['duration'] for t in successful_uploads]
        
        # Check for OOM kills
        max_memory = max(m['memory_mb'] for m in resources)
        avg_memory = statistics.mean(m['memory_mb'] for m in resources)
        max_cpu = max(m['cpu_percent'] for m in resources)
        
        print(f"""
Load Test Results:
==================
Total Uploads: {total_uploads}
Successful: {len(successful_uploads)}
Failed: {len(failed_uploads)}
Avg Response Time: {statistics.mean(response_times):.2f}s
P95 Response Time: {statistics.quantiles(response_times, n=20)[18]:.2f}s
Max Memory: {max_memory:.0f} MB
Avg Memory: {avg_memory:.0f} MB
Max CPU: {max_cpu:.1f}%
OOM Risk: {'HIGH' if max_memory > 15000 else 'LOW'}
        """)
        
        # Verify no OOM kills occurred
        assert max_memory < 16000, f"Memory usage too high: {max_memory} MB"
        assert len(failed_uploads) == 0, f"Failed uploads: {failed_uploads}"

# Run the test
if __name__ == "__main__":
    asyncio.run(run_load_test(concurrent_uploads=5, total_uploads=20))
6. Memory Monitoring for Cloud Run
Add memory monitoring to prevent OOM:
python# resources/cloud-transcoder/memory_monitor.py
import psutil
import os
import gc

class MemoryMonitor:
    def __init__(self, threshold_mb=14000):  # 14GB threshold for 16GB container
        self.threshold_mb = threshold_mb
        
    def check_memory(self):
        """Check current memory usage"""
        memory = psutil.virtual_memory()
        used_mb = memory.used / (1024 * 1024)
        
        if used_mb > self.threshold_mb:
            # Emergency cleanup
            gc.collect()
            
            # Clear FFmpeg cache if exists
            os.system('sync && echo 1 > /proc/sys/vm/drop_caches 2>/dev/null')
            
            return False, f"Memory critical: {used_mb:.0f} MB"
        
        return True, f"Memory OK: {used_mb:.0f} MB"
    
    def can_process_variant(self, resolution):
        """Check if we have enough memory for this variant"""
        # Estimate memory needed (rough)
        memory_needed = {
            '4K': 4000,
            '1440p': 2000,
            '1080p': 1000,
            '720p': 500,
            'default': 300
        }
        
        res_key = '4K' if resolution > 2160 else \
                  '1440p' if resolution > 1080 else \
                  '1080p' if resolution > 720 else \
                  '720p' if resolution > 480 else \
                  'default'
        
        memory = psutil.virtual_memory()
        available_mb = memory.available / (1024 * 1024)
        
        return available_mb > memory_needed[res_key]

# Use in transcoding
monitor = MemoryMonitor()

def generate_variant_safe(input_file, output_dir, profile):
    """Generate variant with memory safety checks"""
    
    # Pre-check memory
    can_process = monitor.can_process_variant(profile['height'])
    if not can_process:
        print(f"⚠️ Skipping {profile['name']} - insufficient memory")
        return False
    
    try:
        result = generate_variant(input_file, output_dir, profile)
        
        # Post-check memory
        ok, msg = monitor.check_memory()
        if not ok:
            print(f"⚠️ Memory warning after {profile['name']}: {msg}")
            gc.collect()
        
        return result
        
    except Exception as e:
        print(f"❌ Failed {profile['name']}: {e}")
        return False
Validation Checklist
✅ Cloud Run Configuration

 Verify 4 vCPU allocation
 Verify 8-16 GiB memory
 Confirm no CPU throttling
 Test with 1-hour timeout

✅ Parallel Processing

 ThreadPoolExecutor with MAX_WORKERS=4
 Resource semaphore prevents OOM
 Priority processing (critical profiles first)
 Progress updates during processing

✅ Buffer Management

 2-minute cleanup interval on mobile
 3-minute cleanup interval on desktop
 Smart cleanup based on buffer size
 Cleanup after quality switches

✅ CMS Integration

 Upload endpoint receives files
 Triggers Cloud Run transcoding
 Returns status URL immediately
 Status polling works end-to-end

✅ Load Testing

 No OOM kills under concurrent load
 Memory stays under 14GB threshold
 All variants generate successfully
 Response times acceptable

Success Metrics

Transcoding Speed: 75% faster with parallel processing
Memory Usage: Peak < 14GB under load (no OOM)
First Playback: Available within 30s (critical profiles)
CMS Integration: Upload → Transcode → Play works end-to-end
Reliability: 99%+ success rate under load

Please implement these optimizations and verify all components work together properly.